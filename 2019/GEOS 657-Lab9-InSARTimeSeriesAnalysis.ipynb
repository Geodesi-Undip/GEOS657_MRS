{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"NotebookAddons/blackboard-banner.png\" width=\"100%\" />\n",
    "<font face=\"Calibri\">\n",
    "<br>\n",
    "<font size=\"7\"> <b> GEOS 657: Microwave Remote Sensing<b> </font>\n",
    "\n",
    "<font size=\"5\"> <b>Lab 9: InSAR Time Series Analysis using GIAnT within Jupyter Notebooks</b> </font>\n",
    "\n",
    "<br>\n",
    "<font size=\"4\"> <b> Franz J Meyer & Joshua J C Knicely; University of Alaska Fairbanks</b> <br>\n",
    "<img src=\"NotebookAddons/UAFLogo_A_647.png\" width=\"170\" align=\"right\" /><font color='rgba(200,0,0,0.2)'> <b>Due Date: </b>NONE</font>\n",
    "</font>\n",
    "\n",
    "<font size=\"3\"> This Lab is part of the UAF course <a href=\"https://radar.community.uaf.edu/\" target=\"_blank\">GEOS 657: Microwave Remote Sensing</a>. The primary goal of this lab is to demonstrate how to process InSAR data, specifically interferograms, using the Generic InSAR Analysis Toolbox (<a href=\"http://earthdef.caltech.edu/projects/giant/wiki\" target=\"_blank\">GIAnT</a>) in the framework of *Jupyter Notebooks*.<br>\n",
    "\n",
    "<b>Our specific objectives for this lab are to:</b>\n",
    "\n",
    "- Learn how to prepare data for GIAnT. \n",
    "- Use GIAnT to create maps of surface deformation. \n",
    "    -  Understand its capabilities. \n",
    "    -  Understand its limitations. \n",
    "</font>\n",
    "\n",
    "<br>\n",
    "<font face=\"Calibri\">\n",
    "\n",
    "<font size=\"5\"> <b> Target Description </b> </font>\n",
    "\n",
    "<font size=\"3\"> In this lab, we will analyze the volcano Sierra Negra. This is a highly active volcano on the Galapagos hotpsot. The most recent eruption occurred from 29 June to 23 August 2018. The previous eruption occurred in October 2005, prior to the launch of the Sentinel-1 satellites, which will be the source of data we use for this lab. We will be looking at the deformation that occurred prior to the volcano's 2018 eruption. </font>\n",
    "\n",
    "<font size=\"4\"> <font color='rgba(200,0,0,0.2)'> <b>THIS NOTEBOOK INCLUDES NO HOMEWORK ASSIGNMENTS.</b></font> <br>\n",
    "\n",
    "Contact me at fjmeyer@alaska.edu should you run into any problems.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50651274ffba44aaa964609b96bf31bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "URLWidget()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import url_widget as url_w\n",
    "notebookUrl = url_w.URLWidget()\n",
    "display(notebookUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "from IPython.display import display\n",
    "\n",
    "notebookUrl = notebookUrl.value\n",
    "user = !echo $JUPYTERHUB_USER\n",
    "env = !echo $CONDA_PREFIX\n",
    "if env[0] == '':\n",
    "    env[0] = 'Python 3 (base)'\n",
    "if env[0] != '/home/jovyan/.local/envs/insar_analysis':\n",
    "    display(Markdown(f'<text style=color:red><strong>WARNING:</strong></text>'))\n",
    "    display(Markdown(f'<text style=color:red>This notebook should be run using the \"insar_analysis\" conda environment.</text>'))\n",
    "    display(Markdown(f'<text style=color:red>It is currently using the \"{env[0].split(\"/\")[-1]}\" environment.</text>'))\n",
    "    display(Markdown(f'<text style=color:red>Select \"insar_analysis\" from the \"Change Kernel\" submenu of the \"Kernel\" menu.</text>'))\n",
    "    display(Markdown(f'<text style=color:red>If the \"insar_analysis\" environment is not present, use <a href=\"{notebookUrl.split(\"/user\")[0]}/user/{user[0]}/notebooks/conda_environments/Create_OSL_Conda_Environments.ipynb\"> Create_OSL_Conda_Environments.ipynb </a> to create it.</text>'))\n",
    "    display(Markdown(f'<text style=color:red>Note that you must restart your server after creating a new environment before it is usable by notebooks.</text>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='5'><b>Overview</b></font>\n",
    "<br>\n",
    "<font size='3'><b>About GIAnT</b>\n",
    "<br>\n",
    "GIAnT is a Python framework that allows rapid time series analysis of low amplitude deformation signals. It allows users to use multiple time series analysis technqiues: Small Baseline Subset (SBAS), New Small Baseline Subset (N-SBAS), and Multiscale InSAR Time-Series (MInTS). As a part of this, it includes the ability to correct for atmospheric delays by assuming a spatially uniform stratified atmosphere. \n",
    "<br><br>\n",
    "<b>Limitations</b>\n",
    "<br>\n",
    "GIAnT has a number of limitations that are important to keep in mind as these can affect its effectiveness for certain applications. It implements the simplest time-series inversion methods. Its single coherence threshold is very conservative in terms of pixel selection. It does not include any consistency checks for unwrapping errors. It has a limited dictionary of temporal model functions. It cannot correct for atmospheric effects due to differing surface elevations. \n",
    "<br><br>\n",
    "<b>Steps to use GIAnT</b><br>\n",
    "Although GIAnT is an incredibly powerful tool, it requires very specific input. Because of the input requirements, the majority of one's effort goes to getting the data into a form that GIAnT can manipulate and to creating files that tell GIAnT what to do. The general steps to use GIAnT are below. \n",
    "\n",
    "- Download Data\n",
    "- Identify Area of Interest\n",
    "- Subset (Crop) Data to Area of Interest\n",
    "- Prepare Data for GIAnT\n",
    "    - Adjust file names\n",
    "    - Remove potentially disruptive default values (optional)\n",
    "    - Convert data from '.tiff' to '.flt' format\n",
    "- Create Input Files for GIAnT\n",
    "    - Create 'ifg.list'\n",
    "    - Create 'date.mli.par'\n",
    "    - Make prepxml_SBAS.py\n",
    "    - Run prepxml_SBAS.py\n",
    "    - Make userfn.py\n",
    "- Run GIAnT\n",
    "    - PrepIgramStack.py*\n",
    "    - ProcessStack.py\n",
    "    - SBASInvert.py\n",
    "    - SBASxval.py\n",
    "- Data Visualization\n",
    "\n",
    "<br>\n",
    "The steps from PrepIgramStack.py and above have been completed for you in order to save disk space and computation time. This allows us to concentrate on the usage of GIAnT and data visualization. Some of the code to create the prepatory files (e.g., 'ifg.list', 'date.mli.par', etc.) have been incldued for your potential use. More information about GIAnT can be found here: (<a href=\"http://earthdef.caltech.edu/projects/giant/wiki\" target=\"_blank\">http://earthdef.caltech.edu/projects/giant/wiki</a>)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\" size=\"5\" color=\"darkred\"> <b>Important Note about JupyterHub</b> </font>\n",
    "<br><br>\n",
    "<font face=\"Calibri\" size=\"3\"> <b>Your JupyterHub server will automatically shutdown when left idle for more than 1 hour. Your notebooks will not be lost but you will have to restart their kernels and re-run them from the beginning. You will not be able to seamlessly continue running a partially run notebook.</b> </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='5'><b>0. Import Python Libraries:</b></font><br><br>\n",
    "<font size='3'><b>Import the Python libraries and modules we will need to run this lab:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from datetime import date\n",
    "import glob\n",
    "import h5py # for is_hdf5\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from osgeo import gdal\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation\n",
    "from matplotlib import rc\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "import opensarlab_lib as asfn\n",
    "asfn.jupytertheme_matplotlib_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='5'><b>1. Transfer data to a local directory</b></font><br>\n",
    "    <font size='3'>The data cube (referred to as a stack in the GIAnT documentation and code) and several other needed files have been created and stored in the GIAnT server. We will download this data to a local directory and unzip it. </font></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"> Before we download anything, <b>create a working directory for this analysis and change into it:</b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/jovyan/GEOS_657_Labs/2019/2019/lab_9_data\n"
     ]
    }
   ],
   "source": [
    "path = f\"{os.getcwd()}/2019/lab_9_data\"\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "os.chdir(path)\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face = 'Calibri' size='3'>First step is to find the zip file and download it to a local directory. This zip file has been placed in the S3 bucket for this class.\n",
    "<br><br>\n",
    "<b>Display the contents of the S3 bucket:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE Elazig/\n",
      "                           PRE Fielding/\n",
      "                           PRE Schultz/\n",
      "                           PRE TOPS/\n",
      "                           PRE aria-data/\n",
      "                           PRE bkubby/\n",
      "                           PRE e_perez/\n",
      "                           PRE fattahi-backup/\n",
      "                           PRE notebook_testing/\n",
      "                           PRE notebook_testing_data/\n",
      "                           PRE osl_themes/\n",
      "                           PRE unavco2021/\n",
      "2021-05-24 22:22:18 3968147750 2020BangladeshStack-Part1.zip\n",
      "2021-05-24 22:21:33 2459723072 2020BangladeshStack-Part2.zip\n",
      "2021-05-23 05:19:59   58328348 2020WaterMasks.zip\n",
      "2021-05-10 22:04:09  625289152 Bangladesh.tar.gz\n",
      "2021-05-10 22:04:14 3219682312 BangladeshFloodMapping.tar.gz\n",
      "2021-05-10 22:04:06  387733571 BangladeshFloodMapping_binder.tar.gz\n",
      "2021-05-10 22:04:35  201607632 BogotaAg.zip\n",
      "2021-05-10 22:04:36 1044602962 Colombia-ROI-PenaDeBarro.zip\n",
      "2021-05-10 22:04:48  826544586 Colombia-ROI-Torro.zip\n",
      "2021-05-10 22:04:43  109816930 E2_80464_STD_F163_dem.tif\n",
      "2021-05-10 22:04:35    4312000 E451_20000818_20020719.unw\n",
      "2021-05-10 22:04:48     186048 GIAnT.zip\n",
      "2021-05-10 22:04:48     187160 GIAnT_5_21.zip\n",
      "2021-05-10 22:04:57   65390716 Lab9Files.zip\n",
      "2021-05-10 22:04:58  656597483 MadreDeDios.zip\n",
      "2021-05-10 22:05:01   88370791 MadreDeDios_binder.zip\n",
      "2021-05-10 22:05:01 2234911289 NISARtraining_inputSFmintpy.zip\n",
      "2021-05-10 22:05:01   11604905 Niamey.zip\n",
      "2021-05-10 22:05:01  864691028 S1-InundationMapping.zip\n",
      "2021-08-03 19:38:48 3144833284 SanFranSenDT42.tar.xz\n",
      "2021-05-10 22:06:19  433650266 TSAmandaElSalvador.tar.gz\n",
      "2021-05-10 22:06:21  332262049 TSAmandaElSalvador_binder.tar.gz\n",
      "2021-05-10 22:06:34  258354949 bangladesh_dem_hand_cop-glo-30.tif\n",
      "2021-05-10 22:06:44 1700723566 bangladesh_dem_hand_f32.tif\n",
      "2021-05-10 22:07:37         44 blank.txt\n",
      "2021-05-10 22:08:01  111419732 flood.tar.gz\n",
      "2021-05-10 22:08:05  503416689 flood_map_apr_2017.tif\n",
      "2021-05-10 22:08:05  503371778 flood_map_aug_2017.tif\n",
      "2021-05-10 22:08:23  503371778 flood_map_jun_2017.tif\n",
      "2021-05-10 22:08:23   64728033 fuego.tar.gz\n",
      "2021-09-15 23:44:57 2715979689 havana_test.zip\n",
      "2021-05-10 22:08:42       5942 test.html\n",
      "2021-05-10 22:08:43 1986328318 time_series.zip\n",
      "2021-05-10 22:08:52  612187856 tropical.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls --region=us-east-1 --no-sign-request s3://asf-jupyter-data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face = 'Calibri' size='3'><b>Copy the desired file ('Lab9Files.zip') to your data directory:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://asf-jupyter-data/Lab9Files.zip to ./Lab9Files.zip  \n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --region=us-east-1 --no-sign-request s3://asf-jupyter-data/Lab9Files.zip ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'><b>Create the directories where we will perform the GIAnT analysis and store the data:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_path = f\"{os.getcwd()}/Stack\" # directory GIAnT prefers to access and store data steps. \n",
    "if not os.path.exists(stack_path):\n",
    "    os.makedirs(stack_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'><b>Extract the zipped file to path and delete it:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting: Lab9Files.zip\n"
     ]
    }
   ],
   "source": [
    "zipped = 'Lab9Files.zip'\n",
    "asfn.asf_unzip(path, zipped)\n",
    "if os.path.exists(zipped):\n",
    "    os.remove(zipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'>The files have been extracted and placed in a folder called 'Lab9Files'. <b>Move the amplitude image, data.xml, date.mli.par, and sbas.xml files to path and RAW-STACK.h5 to stack_path:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dir = f\"{path}/Lab9Files\"\n",
    "if not os.path.exists(f\"{stack_path}/RAW-STACK.h5\"):\n",
    "    shutil.move(f\"{temp_dir}/RAW-STACK.h5\", stack_path) \n",
    "files = glob.glob(f\"{temp_dir}/*.*\")\n",
    "for file in files:\n",
    "    if os.path.exists(file):\n",
    "        shutil.move(file, path)\n",
    "if os.path.exists(temp_dir):\n",
    "    os.rmdir(temp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='5'><b>2. Create Input Files And Code for GIAnT</b></font>\n",
    "    <br>\n",
    "    <font size ='3'>The code below shows how to create the input files and specialty code that GIAnT requires. For this lab, 'ifg.list' is not needed, 'date.mli.par' has already been provided, 'prepxml_SBAS.py' is not needed as the 'sbas.xml' and 'data.xml' files it would create have already been provided, and 'userfn.py' is not needed as we are skipping the step in which it would be used. <br>The files that would be created are listed below. \n",
    "        <br>\n",
    "        \n",
    "- ifg.list\n",
    "    - List of the interferogram properties including master and slave date, perpendicular baseline, and sensor. \n",
    "- date.mli.par\n",
    "    - File from which GIAnT pulls requisite information about the sensor. \n",
    "    - This is specifically for GAMMA files. When using other interferogram processing techniques, an alternate file is required. \n",
    "- prepxml_SBAS.py\n",
    "    - Python function to create an xml file that specifies the processing options to GIAnT. \n",
    "    - This must be modified by the user for their particular application. \n",
    "- userfn.py\n",
    "    - Python function to map the interferogram dates to a phyiscal file on disk. \n",
    "    - This must be modified by the user for their particular application. \n",
    "    </font>\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='4'> <b>2.1 Create 'ifg.list' File </b> </font> </font>\n",
    "<br>\n",
    "<font face='Calibri' size='3'> This will create simple 4 column text file will communicate network information to GIAnT. It will be created within the <b>GIAnT</b> folder.\n",
    "<br><br>\n",
    "<b>This step has already been done, so we will not actually create the 'ifg.list' file. This code is displayed for your potential future use.</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Get one of each file name. This assumes the unwrapped phase geotiff has been converted to a '.flt' file\\nfiles = [f for f in os.listdir(datadirectory) if f.endswith('_unw_phase.flt')] \\n\\n# Get all of the master and slave dates. \\nmasterDates,slaveDates = [],[]\\nfor file in files:\\n    masterDates.append(file[0:8])\\n    slaveDates.append(file[9:17])\\n# Sort the dates according to the master dates. \\nmaster_dates,sDates = (list(t) for t in zip(*sorted(zip(masterDates,slaveDates))))\\n\\nwith open( os.path.join('GIAnT', 'ifg.list'), 'w') as fid:\\n    for i in range(len(master_dates)):\\n        masterDate = master_dates[i] # pull out master Date (first set of numbers)\\n        slaveDate = sDates[i] # pull out slave Date (second set of numbers)\\n        bperp = '0.0' # according to JPL notebooks\\n        sensor = 'S1' # according to JPL notebooks\\n        fid.write(f'{masterDate}  {slaveDate}  {bperp}  {sensor}\\n') # write values to the 'ifg.list' file. \\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Get one of each file name. This assumes the unwrapped phase geotiff has been converted to a '.flt' file\n",
    "files = [f for f in os.listdir(datadirectory) if f.endswith('_unw_phase.flt')] \n",
    "\n",
    "# Get all of the master and slave dates. \n",
    "masterDates,slaveDates = [],[]\n",
    "for file in files:\n",
    "    masterDates.append(file[0:8])\n",
    "    slaveDates.append(file[9:17])\n",
    "# Sort the dates according to the master dates. \n",
    "master_dates,sDates = (list(t) for t in zip(*sorted(zip(masterDates,slaveDates))))\n",
    "\n",
    "with open( os.path.join('GIAnT', 'ifg.list'), 'w') as fid:\n",
    "    for i in range(len(master_dates)):\n",
    "        masterDate = master_dates[i] # pull out master Date (first set of numbers)\n",
    "        slaveDate = sDates[i] # pull out slave Date (second set of numbers)\n",
    "        bperp = '0.0' # according to JPL notebooks\n",
    "        sensor = 'S1' # according to JPL notebooks\n",
    "        fid.write(f'{masterDate}  {slaveDate}  {bperp}  {sensor}\\n') # write values to the 'ifg.list' file. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'>You may notice that the code above sets the perpendicular baseline to a value of 0.0 m. This is not the true perpendicular baseline. That value can be found in metadata file (titled '$<$master timestamp$>$_$<$slave timestamp$>$.txt') that comes with the original interferogram. Generally, we would want the true baseline for each interferogram. However, since Sentinel-1 has such a short baseline, a value of 0.0 m is sufficient for our purposes. </font></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='4'> <b>2.2 Create 'date.mli.par' File </b></font> \n",
    "<br>\n",
    "<font face='Calibri' size='3'>As we are using GAMMA products, we must create a 'date.mli.par' file from which GIAnT will pull necessary information. If another processing technique is used to create the interferograms, an alterante file name and file inputs are required.\n",
    "<br><br>\n",
    "<b>Again, this step has already been completed and the code is only displayed for your potential future use.</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Create file 'date.mli.par'\\n\\n# Get file names\\nfiles = [f for f in os.listdir(datadirectory) if f.endswith('_unw_phase.flt')]\\n\\n# Get WIDTH (xsize) and FILE_LENGTH (ysize) information\\nds = gdal.Open(datadirectory+files[0], gdal.GA_ReadOnly)\\ntype(ds)\\n\\nnLines = ds.RasterYSize\\nnPixels = ds.RasterXSize\\n\\ntrans = ds.GetGeoTransform()\\nds = None\\n\\n# Get the center line UTC time stamp; can also be found inside <date>_<date>.txt file and hard coded\\ndirName = os.listdir('ingrams')[0] # get original file name (any file can be used; the timestamps are different by a few seconds)\\nvals = dirName.split('-') # break file name into parts using the separator '-'\\ntstamp = vals[2][9:16] # extract the time stamp from the 2nd datetime (could be the first)\\nc_l_utc = int(tstamp[0:2])*3600 + int(tstamp[2:4])*60 + int(tstamp[4:6])\\n\\nrfreq = 299792548.0 / 0.055465763 # radar frequency; speed of light divided by radar wavelength of Sentinel1 in meters\\n\\n# write the 'date.mli.par' file\\nwith open(os.path.join(path, 'date.mli.par'), 'w') as fid:\\n    # Method 1\\n    fid.write(f'radar_frequency: {rfreq} \\n') # when using GAMMA products, GIAnT requires the radar frequency. Everything else is in wavelength (m) \\n    fid.write(f'center_time: {c_l_utc} \\n') # Method from Tom Logan's prepGIAnT code; can also be found inside <date>_<date>.txt file and hard coded\\n    fid.write( 'heading: -11.9617913 \\n') # inside <date>_<date>.txt file; can be hardcoded or set up so code finds it. \\n    fid.write(f'azimuth_lines: {nLines} \\n') # number of lines in direction of the satellite's flight path\\n    fid.write(f'range_samples: {nPixels} \\n') # number of pixels in direction perpendicular to satellite's flight path\\n    fid.close() # close the file\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Create file 'date.mli.par'\n",
    "\n",
    "# Get file names\n",
    "files = [f for f in os.listdir(datadirectory) if f.endswith('_unw_phase.flt')]\n",
    "\n",
    "# Get WIDTH (xsize) and FILE_LENGTH (ysize) information\n",
    "ds = gdal.Open(datadirectory+files[0], gdal.GA_ReadOnly)\n",
    "type(ds)\n",
    "\n",
    "nLines = ds.RasterYSize\n",
    "nPixels = ds.RasterXSize\n",
    "\n",
    "trans = ds.GetGeoTransform()\n",
    "ds = None\n",
    "\n",
    "# Get the center line UTC time stamp; can also be found inside <date>_<date>.txt file and hard coded\n",
    "dirName = os.listdir('ingrams')[0] # get original file name (any file can be used; the timestamps are different by a few seconds)\n",
    "vals = dirName.split('-') # break file name into parts using the separator '-'\n",
    "tstamp = vals[2][9:16] # extract the time stamp from the 2nd datetime (could be the first)\n",
    "c_l_utc = int(tstamp[0:2])*3600 + int(tstamp[2:4])*60 + int(tstamp[4:6])\n",
    "\n",
    "rfreq = 299792548.0 / 0.055465763 # radar frequency; speed of light divided by radar wavelength of Sentinel1 in meters\n",
    "\n",
    "# write the 'date.mli.par' file\n",
    "with open(os.path.join(path, 'date.mli.par'), 'w') as fid:\n",
    "    # Method 1\n",
    "    fid.write(f'radar_frequency: {rfreq} \\n') # when using GAMMA products, GIAnT requires the radar frequency. Everything else is in wavelength (m) \n",
    "    fid.write(f'center_time: {c_l_utc} \\n') # Method from Tom Logan's prepGIAnT code; can also be found inside <date>_<date>.txt file and hard coded\n",
    "    fid.write( 'heading: -11.9617913 \\n') # inside <date>_<date>.txt file; can be hardcoded or set up so code finds it. \n",
    "    fid.write(f'azimuth_lines: {nLines} \\n') # number of lines in direction of the satellite's flight path\n",
    "    fid.write(f'range_samples: {nPixels} \\n') # number of pixels in direction perpendicular to satellite's flight path\n",
    "    fid.close() # close the file\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='4'><b>2.3 Make prepxml_SBAS.py</b> </font>\n",
    "<br>\n",
    "<font size='3'>We will create a prepxml_SBAS.py function and put it into our GIAnT working directory. Again, this is shown for anyone that may want to use GIAnT on their own.<br>If we do wish to change 'sbas.xml' or 'data.xml', this can be done by creating and running a new 'prepxml_SBAS.py'. </font>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'> <font size='3'><b>2.3.1 Necessary prepxml_SBAS.py edits</b></font>\n",
    "<br>\n",
    "<font size='3'> GIAnT comes with an example prepxml_SBAS.py, but requries significant edits for our purposes. These alterations have already been made, so we don't have to do anything now, but it is good to know the kinds of things that have to be altered. The details of some of these options can be found in the GIAnT documentation. The rest must be found in the GIAnT processing files themselves, most notably the tsxml.py and tsio.py functions. <br>The following alterations were made:\n",
    "<br>\n",
    "- Changed 'example' &#9658; 'date.mli.par'\n",
    "- Removed 'xlim', 'ylim', 'ref_x_lim', and 'ref_y_lim'\n",
    "    - These are used for clipping the files in GIAnT. As we have already done this, it is not necessary. \n",
    "- Removed latfile='lat.map' and lonfile='lon.map'\n",
    "    - These are optional inputs for the latitude and longitude maps. \n",
    "- Removed hgtfile='hgt.map'\n",
    "    - This is an optional altitude file for the sensor. \n",
    "- Removed inc=21.\n",
    "    - This is the optional incidence angle information. \n",
    "    - It can be a constant float value or incidence angle file. \n",
    "    - For Sentinel1, it varies from 29.1-46.0&deg;.\n",
    "- Removed masktype='f4'\n",
    "    - This is the mask designation. \n",
    "    - We are not using any masks for this. \n",
    "- Changed unwfmt='RMG' &#9658; unwfmt='GRD'\n",
    "    - Read data using GDAL. \n",
    "- Removed demfmt='RMG'\n",
    "- Changed corfmt='RMG' &#9658; corfmt='GRD'\n",
    "    - Read data using GDAL. \n",
    "- Changed nvalid=30 -> nvalid=1\n",
    "    - This is the minimum number of interferograms in which a pixel must be coherent. A particular pixel will be included only if its coherence is above the coherence threshold, cohth, in more than nvalid number of interferograms. \n",
    "- Removed atmos='ECMWF'\n",
    "    - This is an amtospheric correction command. It depends on a library called 'pyaps' developed for GIAnT. This library has not been installed yet. \n",
    "- Changed masterdate='19920604' &#9658; masterdate='20161119'\n",
    "    - Use our actual masterdate. \n",
    "    - I simply selected the earliest date as the masterdate. \n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'>Defining a reference region is a potentially important step. This is a region at which there should be no deformation. For a volcano, this should be some significant distance away from the volcano. GIAnT has the ability to automatically select a reference region which we will use for this exercise. <br>Below is an example of how the reference region would be defined. If we look at the prepxml_SBAS.py code below, ref_x_lim and ref_y_lim, the pixel based location of the reference region, is within the code, but has been commented out. \n",
    "<br><br>\n",
    "<b>Define reference region:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_x_lim, ref_y_lim = [0, 10], [95, 105]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'>Below is an example of how the reference region would be defined. Look at the prepxml_SBAS.py code below. Note that ref_x_lim and ref_y_lim (the pixel based location of the reference region) are within the code.\n",
    "<br><br>\n",
    "<b>This has already been completed but the code is here as an example script for creating XML files for use with the SBAS processing chain.</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#!/usr/bin/env python\\n\\nimport tsinsar as ts\\nimport argparse\\nimport numpy as np\\n\\ndef parse():\\n    parser= argparse.ArgumentParser(description='Preparation of XML files for setting up the processing chain. Check tsinsar/tsxml.py for details on the parameters.')\\n    parser.parse_args()\\n\\nparse()\\ng = ts.TSXML('data')\\ng.prepare_data_xml(\\n    'date.mli.par', proc='GAMMA', \\n    #ref_x_lim = [{1},{2}], ref_y_lim=[{3},{4}],\\n    inc = 21., cohth=0.10, \\n    unwfmt='GRD', corfmt='GRD', chgendian='True', endianlist=['UNW','COR'])\\ng.writexml('data.xml')\\n\\n\\ng = ts.TSXML('params')\\ng.prepare_sbas_xml(nvalid=1, netramp=True, demerr=False, uwcheck=False, regu=True, masterdate='{5}', filt=1.0)\\ng.writexml('sbas.xml')\\n\\n\\n############################################################\\n# Program is part of GIAnT v1.0                            #\\n# Copyright 2012, by the California Institute of Technology#\\n# Contact: earthdef@gps.caltech.edu                        #\\n############################################################\\n\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import tsinsar as ts\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "def parse():\n",
    "    parser= argparse.ArgumentParser(description='Preparation of XML files for setting up the processing chain. Check tsinsar/tsxml.py for details on the parameters.')\n",
    "    parser.parse_args()\n",
    "\n",
    "parse()\n",
    "g = ts.TSXML('data')\n",
    "g.prepare_data_xml(\n",
    "    'date.mli.par', proc='GAMMA', \n",
    "    #ref_x_lim = [{1},{2}], ref_y_lim=[{3},{4}],\n",
    "    inc = 21., cohth=0.10, \n",
    "    unwfmt='GRD', corfmt='GRD', chgendian='True', endianlist=['UNW','COR'])\n",
    "g.writexml('data.xml')\n",
    "\n",
    "\n",
    "g = ts.TSXML('params')\n",
    "g.prepare_sbas_xml(nvalid=1, netramp=True, demerr=False, uwcheck=False, regu=True, masterdate='{5}', filt=1.0)\n",
    "g.writexml('sbas.xml')\n",
    "\n",
    "\n",
    "############################################################\n",
    "# Program is part of GIAnT v1.0                            #\n",
    "# Copyright 2012, by the California Institute of Technology#\n",
    "# Contact: earthdef@gps.caltech.edu                        #\n",
    "############################################################\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>Set the master date and create a script for creating XML files for use with the SBAS processing chain: </b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#files = [f for f in os.listdir(datadirectory) if f.endswith('_unw_phase.flt')]\n",
    "#master_date = min([files[i][0:8] for i in range(len(files))], key=int)\n",
    "\n",
    "master_date = '20161119'\n",
    "\n",
    "prepxml_SBAS_Template = '''\n",
    "#!/usr/bin/env python\n",
    "\"\"\"Example script for creating XML files for use with the SBAS processing chain. This script is supposed to be copied to the working directory and modified as needed.\"\"\"\n",
    "\n",
    "\n",
    "import tsinsar as ts\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "def parse():\n",
    "    parser= argparse.ArgumentParser(description='Preparation of XML files for setting up the processing chain. Check tsinsar/tsxml.py for details on the parameters.')\n",
    "    parser.parse_args()\n",
    "\n",
    "parse()\n",
    "g = ts.TSXML('data')\n",
    "g.prepare_data_xml(\n",
    "    'date.mli.par', proc='GAMMA', \n",
    "    #ref_x_lim = [{1},{2}], ref_y_lim=[{3},{4}],\n",
    "    inc = 21., cohth=0.10, \n",
    "    unwfmt='GRD', corfmt='GRD', chgendian='True', endianlist=['UNW','COR'])\n",
    "g.writexml('data.xml')\n",
    "\n",
    "\n",
    "g = ts.TSXML('params')\n",
    "g.prepare_sbas_xml(nvalid=1, netramp=True, demerr=False, uwcheck=False, regu=True, masterdate='{5}', filt=1.0)\n",
    "g.writexml('sbas.xml')\n",
    "\n",
    "\n",
    "############################################################\n",
    "# Program is part of GIAnT v1.0                            #\n",
    "# Copyright 2012, by the California Institute of Technology#\n",
    "# Contact: earthdef@gps.caltech.edu                        #\n",
    "############################################################\n",
    "\n",
    "'''\n",
    "with open(os.path.join(path,'prepxml_SBAS.py'), 'w') as fid:\n",
    "    fid.write(prepxml_SBAS_Template.format(path,ref_x_lim[0],ref_x_lim[1],ref_y_lim[0],ref_y_lim[1],master_date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'>To create a new 'sbas.xml' and 'data.xml' file, we would modify the above code to give new parameters and to write to the appropriate folder (e.g., to change the time filter from 1 year to none and to write to the directory in which we are working; 'filt=1.0' -> 'filt=0.0'; and 'os.path.join(path,'prepxml_SBAS.py') -> 'prepxml_SBAS.py' OR '%cd ~' into your home directory). Then we would run it below. </font></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='4'> <b>2.4 Run prepxml_SBAS.py </b> </font>\n",
    "<br>\n",
    "<font face='Calibri' size='3'> Here we run <b>prepxml_SBAS.py</b> to create the 2 needed files</font>\n",
    "\n",
    "- data.xml \n",
    "- sbas.xml\n",
    "\n",
    "<font face='Calibri' size='3'> To use MinTS, we would run <b>prepxml_MinTS.py</b> to create</font>\n",
    "\n",
    "- data.xml\n",
    "- mints.xml\n",
    "        \n",
    "<font face='Calibri' size='3'> These files are needed by <b>PrepIgramStack.py</b>. \n",
    "<br>\n",
    "We must first switch to the GIAnT folder in which <b>prepxml_SBAS.py</b> is contained, then call it. Otherwise, <b>prepxml_SBAS.py</b> will not be able to find the file 'date.mli.par', which holds necessary processing information. \n",
    "<br><br>\n",
    "<b>Create a variable holding the general path to the GIAnT code base and download GIAnT from the asf-jupyter-data S3 bucket, if not present.</b>\n",
    "    <br>\n",
    "    GIAnT is no longer supported (Python 2). This unofficial version of GIAnT has been partially ported to Python 3 to run this notebook. Only the portions of GIAnT used in this notebook have been tested.\n",
    "</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "giant_path = \"/home/jovyan/.local/GIAnT/SCR\"\n",
    "\n",
    "if not os.path.exists(\"/home/jovyan/.local/GIAnT\"):\n",
    "    download_path = 's3://asf-jupyter-data/GIAnT_5_21.zip'\n",
    "    output_path = f\"/home/jovyan/.local/{os.path.basename(download_path)}\"\n",
    "    !aws --region=us-east-1 --no-sign-request s3 cp $download_path $output_path\n",
    "    if os.path.isfile(output_path):\n",
    "        !unzip $output_path -d /home/jovyan/.local/\n",
    "        os.remove(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>Run prepxml_SBAS.py and check the output to confirm that your input values are correct:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !python $giant_path/prepxml_SBAS.py # this has already been done. data.xml and sbas.xml already exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>Make sure the two requisite xml files (data.xml and sbas.xml) were produced after running prepxml_SBAS.py.</b></font>\n",
    "<br><br>\n",
    "<font face='Calibri' size='3'><b>Display the contents of data.xml:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<data>\n",
      "  <proc>\n",
      "    <value>GAMMA</value>\n",
      "    <type>STR</type>\n",
      "    <help>Processor used for generating the interferograms.</help>\n",
      "  </proc>\n",
      "  <master>\n",
      "    <width>\n",
      "      <value>612</value>\n",
      "      <type>INT</type>\n",
      "      <help>WIDTH of the IFGs to be read in.</help>\n",
      "    </width>\n",
      "    <file_length>\n",
      "      <value>415</value>\n",
      "      <type>INT</type>\n",
      "      <help>FILE_LENGTH of the IFGS to be read in.</help>\n",
      "    </file_length>\n",
      "    <wavelength>\n",
      "      <value>0.055465763</value>\n",
      "      <type>FLOAT</type>\n",
      "      <help>WAVELENGTH of the Stack. If combining sensors,ensure that they are all converted to same units.</help>\n",
      "    </wavelength>\n",
      "    <heading>\n",
      "      <value>-11.9617913</value>\n",
      "      <type>FLOAT</type>\n",
      "      <help>Heading of the satellite in degrees. Manually adjust for Doris.</help>\n",
      "    </heading>\n",
      "    <utc>\n",
      "      <value>1585</value>\n",
      "      <type>INT</type>\n",
      "      <help>Average time of acquisition in seconds of the day</help>\n",
      "    </utc>\n",
      "    <mask>\n",
      "      <value></value>\n",
      "      <type>STR</type>\n",
      "      <help>Common mask applied to all the images. Useful forfiltering out water bodies.</help>\n",
      "    </mask>\n",
      "    <cthresh>\n",
      "      <value>0.1</value>\n",
      "      <type>FLOAT</type>\n",
      "      <help>Coherence threshold for pixels to be included in analysis.</help>\n",
      "    </cthresh>\n",
      "    <latfile>\n",
      "      <value></value>\n",
      "      <type>STR</type>\n",
      "      <help>Latitude file. Same size as IFGs.</help>\n",
      "    </latfile>\n",
      "    <lonfile>\n",
      "      <value></value>\n",
      "      <type>STR</type>\n",
      "      <help>Longitude file. Same size as IFGs.</help>\n",
      "    </lonfile>\n",
      "    <hgtfile>\n",
      "      <value></value>\n",
      "      <type>STR</type>\n",
      "      <help>DEM height in radar coordinates. Same size as IFGs.</help>\n",
      "    </hgtfile>\n",
      "    <incidence>\n",
      "      <value>21.0</value>\n",
      "      <type>FLOAT</type>\n",
      "      <help>Incidence angle file. Same size as IFGs. Can be a constant float too.</help>\n",
      "    </incidence>\n",
      "  </master>\n",
      "  <subimage>\n",
      "    <looks>\n",
      "      <value>1</value>\n",
      "      <type>INT</type>\n",
      "      <help>Number of additional looks to be applied for time-series analysis</help>\n",
      "    </looks>\n",
      "    <width>\n",
      "      <value>612</value>\n",
      "      <type>INT</type>\n",
      "      <help>WIDTH of the analyzed image</help>\n",
      "    </width>\n",
      "    <xmin>\n",
      "      <value>0</value>\n",
      "      <type>INT</type>\n",
      "      <help>START of the valid region in X. After taking looks.</help>\n",
      "    </xmin>\n",
      "    <xmax>\n",
      "      <value>612</value>\n",
      "      <type>INT</type>\n",
      "      <help>END of the valid region in X. After taking looks.</help>\n",
      "    </xmax>\n",
      "    <length>\n",
      "      <value>415</value>\n",
      "      <type>INT</type>\n",
      "      <help>LENGTH of the cropped image</help>\n",
      "    </length>\n",
      "    <ymin>\n",
      "      <value>0</value>\n",
      "      <type>INT</type>\n",
      "      <help>START of the crop region in Y. After taking looks.</help>\n",
      "    </ymin>\n",
      "    <ymax>\n",
      "      <value>415</value>\n",
      "      <type>INT</type>\n",
      "      <help>END of the crop region in Y. After taking looks.</help>\n",
      "    </ymax>\n",
      "    <rxmin>\n",
      "      <value>0</value>\n",
      "      <type>INT</type>\n",
      "      <help>Start of reference region in X. After multilooking and cropping.</help>\n",
      "    </rxmin>\n",
      "    <rxmax>\n",
      "      <value>-1</value>\n",
      "      <type>INT</type>\n",
      "      <help>End of reference region in X. After multilooking and cropping.</help>\n",
      "    </rxmax>\n",
      "    <rymin>\n",
      "      <value>0</value>\n",
      "      <type>INT</type>\n",
      "      <help>Start of reference region in Y. After multilooking and cropping.</help>\n",
      "    </rymin>\n",
      "    <rymax>\n",
      "      <value>-1</value>\n",
      "      <type>INT</type>\n",
      "      <help>End of reference region in Y. After multilooking and cropping.</help>\n",
      "    </rymax>\n",
      "    <latfile>\n",
      "      <value>lat.flt</value>\n",
      "      <type>STR</type>\n",
      "      <help>Latitude file for the cropped and multilooked image.</help>\n",
      "    </latfile>\n",
      "    <lonfile>\n",
      "      <value>lon.flt</value>\n",
      "      <type>STR</type>\n",
      "      <help>Longitude file for the cropped and multilooked image.</help>\n",
      "    </lonfile>\n",
      "    <incidence>\n",
      "      <value>21.0</value>\n",
      "      <type>FLOAT</type>\n",
      "      <help>Incidence angle file for the cropped and multilooked image.</help>\n",
      "    </incidence>\n",
      "    <hgtfile>\n",
      "      <value>hgt.flt</value>\n",
      "      <type>STR</type>\n",
      "      <help>Height file for the cropped and multilooked image.</help>\n",
      "    </hgtfile>\n",
      "  </subimage>\n",
      "  <dirs>\n",
      "    <h5dir>\n",
      "      <value>Stack</value>\n",
      "      <type>STR</type>\n",
      "      <help>Directory for storing all the HDF5 files.</help>\n",
      "    </h5dir>\n",
      "    <atmosdir>\n",
      "      <value>Atmos</value>\n",
      "      <type>STR</type>\n",
      "      <help>Directory for storing all the weather model data.</help>\n",
      "    </atmosdir>\n",
      "    <figsdir>\n",
      "      <value>Figs</value>\n",
      "      <type>STR</type>\n",
      "      <help>Directory for storing all the PNG previews at intermediate stages.</help>\n",
      "    </figsdir>\n",
      "    <respdir>\n",
      "      <value>RESP</value>\n",
      "      <type>STR</type>\n",
      "      <help>Directory for storing wavelet impulse responses.</help>\n",
      "    </respdir>\n",
      "  </dirs>\n",
      "  <format>\n",
      "    <unwfmt>\n",
      "      <value>GRD</value>\n",
      "      <type>STR</type>\n",
      "      <help>Unwrapped file format. FLT/RMG</help>\n",
      "    </unwfmt>\n",
      "    <corfmt>\n",
      "      <value>GRD</value>\n",
      "      <type>STR</type>\n",
      "      <help>Coherence file format. FLT/RMG</help>\n",
      "    </corfmt>\n",
      "    <demfmt>\n",
      "      <value>RMG</value>\n",
      "      <type>STR</type>\n",
      "      <help>DEM file format. FLT/RMG</help>\n",
      "    </demfmt>\n",
      "    <chgendian>\n",
      "      <value>True</value>\n",
      "      <type>BOOL</type>\n",
      "      <help>Convert Endianness when reading in data.</help>\n",
      "    </chgendian>\n",
      "    <endianlist>\n",
      "      <value>[\"UNW\", \"COR\"]</value>\n",
      "      <type>LIST</type>\n",
      "      <help>List for changing endianness. Possible Entries:UNW,COR,HGT,LAT,LON,INC,MASK</help>\n",
      "    </endianlist>\n",
      "    <masktype>\n",
      "      <value>f4</value>\n",
      "      <type>STR</type>\n",
      "      <help>Mask datatype.</help>\n",
      "    </masktype>\n",
      "  </format>\n",
      "</data>\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('data.xml'):\n",
    "    !cat data.xml "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>Display the contents of sbas.xml:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<params>\n",
      "  <proc>\n",
      "    <nvalid>\n",
      "      <value>1</value>\n",
      "      <type>INT</type>\n",
      "      <help>Minimum number of coherent IFGs for a single pixel. If zero, pixel should be coherent in all IFGs.</help>\n",
      "    </nvalid>\n",
      "    <uwcheck>\n",
      "      <value>False</value>\n",
      "      <type>BOOL</type>\n",
      "    </uwcheck>\n",
      "    <netramp>\n",
      "      <value>True</value>\n",
      "      <type>BOOL</type>\n",
      "      <help>Network deramp. Remove ramps from IFGs in a network sense.</help>\n",
      "    </netramp>\n",
      "    <gpsramp>\n",
      "      <value>False</value>\n",
      "      <type>BOOL</type>\n",
      "      <help>GPS deramping. Use GPS network information to correct ramps.</help>\n",
      "    </gpsramp>\n",
      "    <stnlist>\n",
      "      <value></value>\n",
      "      <type>STR</type>\n",
      "      <help>Station list for position of GPS stations.</help>\n",
      "    </stnlist>\n",
      "    <stntype>\n",
      "      <value></value>\n",
      "      <type>STR</type>\n",
      "      <help>Type of station list. False for Sopac, True for (Name,Lat,Lon), velocity for (Name,lat,lon,ve,vn,vu)</help>\n",
      "    </stntype>\n",
      "    <gpspath>\n",
      "      <value></value>\n",
      "      <type>STR</type>\n",
      "      <help>Directory that stores the files for SOPAC or full path of the velotable.</help>\n",
      "    </gpspath>\n",
      "    <gpstype>\n",
      "      <value></value>\n",
      "      <type>STR</type>\n",
      "      <help>Type of data that is provided. Can be sopac or velocity.</help>\n",
      "    </gpstype>\n",
      "    <gpsvert>\n",
      "      <value>False</value>\n",
      "      <type>BOOL</type>\n",
      "      <help>Use vertical components from the GPS data.</help>\n",
      "    </gpsvert>\n",
      "    <gpsmodel>\n",
      "      <value>False</value>\n",
      "      <type>BOOL</type>\n",
      "      <help>Use SOPAC model header for the GPS stations.</help>\n",
      "    </gpsmodel>\n",
      "    <gpspreproc>\n",
      "      <value>False</value>\n",
      "      <type>BOOL</type>\n",
      "      <help>Preprocess and smoothen the data using before using.</help>\n",
      "    </gpspreproc>\n",
      "    <gpspad>\n",
      "      <value>3</value>\n",
      "      <type>INT</type>\n",
      "      <help>Number of pixels around GPS station to be averaged for comparison.</help>\n",
      "    </gpspad>\n",
      "    <gpsmin>\n",
      "      <value>5</value>\n",
      "      <type>INT</type>\n",
      "      <help>Minimum number of GPS needed for ramp correction.</help>\n",
      "    </gpsmin>\n",
      "    <atmos>\n",
      "      <value></value>\n",
      "      <type>STR</type>\n",
      "      <help>Atmospheric correction using weather models.</help>\n",
      "    </atmos>\n",
      "    <demerr>\n",
      "      <value>False</value>\n",
      "      <type>BOOL</type>\n",
      "      <help>Correct for DEM Error. Use in case when IFGs</help>\n",
      "    </demerr>\n",
      "    <regularize>\n",
      "      <value>True</value>\n",
      "      <type>BOOL</type>\n",
      "      <help>Regularization of the inverted time-series, if</help>\n",
      "    </regularize>\n",
      "    <masterdate>\n",
      "      <value>20161119</value>\n",
      "      <type>STR</type>\n",
      "      <help>SAR acquisition to be chosen as time zero. If none, first acquisition is used. </help>\n",
      "    </masterdate>\n",
      "    <filterlen>\n",
      "      <value>1.0</value>\n",
      "      <type>FLOAT</type>\n",
      "      <help>Width of the Gaussian filter used for smoothing final time-series</help>\n",
      "    </filterlen>\n",
      "    <tropomin>\n",
      "      <value>1</value>\n",
      "      <type>INT</type>\n",
      "      <help>Minimum scale to be analyzed for empirical tropospheric corrections.</help>\n",
      "    </tropomin>\n",
      "    <tropomax>\n",
      "      <value>None</value>\n",
      "      <type>INT</type>\n",
      "      <help>Maximum scale to be analyzed for empirical tropospheric corrections.</help>\n",
      "    </tropomax>\n",
      "    <tropolooks>\n",
      "      <value>8</value>\n",
      "      <type>INT</type>\n",
      "      <help>Number of additional looks to be applied before estimating tropospheric parameters.</help>\n",
      "    </tropolooks>\n",
      "  </proc>\n",
      "</params>\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('sbas.xml'):\n",
    "    !cat sbas.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='4'><b>2.5 Create userfn.py</b></font>\n",
    "<br>\n",
    "<font size='3'>Before running the next piece of code, <b>PrepIgramStack.py</b>, we must create a python file called <b>userfn.py</b>. This file maps the interferogram dates to a physical file on disk. This python file must be in our working directory, <b>/GIAnT</b>. We can create this file from within the notebook using python. \n",
    "<br><br>\n",
    "<b>Again, this step has already been preformed and is unnecessary, but the code is provided as an example.</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "userfnTemplate = \"\"\"\n",
    "#!/usr/bin/env python\n",
    "import os \n",
    "\n",
    "def makefnames(dates1, dates2, sensor):\n",
    "    dirname = '{0}'\n",
    "    root = os.path.join(dirname, dates1+'-'+dates2)\n",
    "    #unwname = root+'_unw_phase.flt' # for potentially disruptive default values kept. \n",
    "    unwname = root+'_unw_phase_no_default.flt' # for potentially disruptive default values removed. \n",
    "    corname = root+'_corr.flt'\n",
    "    return unwname, corname\n",
    "\"\"\"\n",
    "\n",
    "with open('userfn.py', 'w') as fid:\n",
    "    fid.write(userfnTemplate.format(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='5'><b>3. Run GIAnT</b></font>\n",
    "    <br>\n",
    "    <font size='3'>We have now created all of the necessary files to run GIAnT. The full GIAnT process requires 3 function calls.\n",
    "- PrepIgramStack.py\n",
    "    - After PrepIgramStack.py, we will actually start running GIAnT. \n",
    "- ProcessStack.py\n",
    "- SBASInvert.py\n",
    "- SBASxval.py\n",
    "    - This 4th function call is not necessary and we will skip it, but provides some error estimation that can be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='4'> <b>3.1 Run PrepIgramStack.py </b> </font>\n",
    "<br>\n",
    "<font face='Calibri' size='3'> Here we would run <b>PrepIgramStack.py</b> to create the files for GIAnT. This would read in the input data and the files we previously created and output an HDF5 file. As we do not actually need to call this, it is currently set up to display some help information.<br>\n",
    "Inputs:       \n",
    "- ifg.list\n",
    "- data.xml\n",
    "- sbas.xml  \n",
    "- interferograms\n",
    "- coherence files        \n",
    "\n",
    "Outputs:\n",
    "- RAW-STACK.h5\n",
    "- PNG previews under 'GIAnT/Figs/Igrams'\n",
    "    \n",
    "</font>\n",
    "<br>\n",
    "<font size='3'><b>Display some help information for PrepIgramStack.py:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logger - INFO - GIANT Toolbox - v 1.0\n",
      "logger - INFO - ---------------------\n",
      "usage: PrepIgramStack.py [-h] [-i INAME] [-o ONAME] [-x DXML] [-f FIGS]\n",
      "                         [-u USERPY]\n",
      "\n",
      "Reads in unwrapped interferograms and prepares data from time-series\n",
      "processing. See doc strings in code for details.\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help  show this help message and exit\n",
      "  -i INAME    Text file to be read for interferogram information. Default:\n",
      "              ifg.list\n",
      "  -o ONAME    To override the default output HDF5 file. Default: RAW-STACK.h5\n",
      "  -x DXML     To override the default data xml file. Default: data.xml\n",
      "  -f FIGS     To override the default directory for creating PNG previews.\n",
      "              Default:Igrams\n",
      "  -u USERPY   Stand-alone python script with user-defined function\n",
      "              (makefnames).\n"
     ]
    }
   ],
   "source": [
    "!python $giant_path/PrepIgramStack.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='3'><b>Run PrepIgramStack.py (in our case, this has already been done):</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python $giant_path/PrepIgramStack.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face='Calibri'><font size='3'>PrepIgramStack.py creates a file called 'RAW-STACK.h5'.\n",
    "<br><br>\n",
    "<b>Verify that RAW-STACK.h5 is an HDF5 file as required by the rest of GIAnT.</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confirmed: /home/jovyan/GEOS_657_Labs/2019/2019/lab_9_data/Stack/RAW-STACK.h5 is an HDF5 file.\n"
     ]
    }
   ],
   "source": [
    "raw_h5 = f\"{stack_path}/RAW-STACK.h5\"\n",
    "if not h5py.is_hdf5(raw_h5):\n",
    "    print(f\"Not an HDF5 file: {raw_h5}\")\n",
    "else:\n",
    "    print(f\"Confirmed: {raw_h5} is an HDF5 file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='4'> <b>3.2 Run ProcessStack.py </b> </font>\n",
    "<br>\n",
    "<font face='Calibri' size='3'> This seems to be an optional step. Does atmospheric corrections and estimation of orbit residuals. <br>\n",
    "Inputs:\n",
    "\n",
    "- HDF5 files from PrepIgramStack.py, RAW-STACK.h5\n",
    "- data.xml \n",
    "- sbas.xml\n",
    "- GPS Data (optional; we don't have this)\n",
    "- Weather models (downloaded automatically)\n",
    "\n",
    "Outputs: \n",
    "\n",
    "- HDF5 files, PROC-STACK.h5\n",
    "        \n",
    "These files are then fed into SBAS. \n",
    "</font> \n",
    "<br><br>\n",
    "<font face='Calibri' size='3'><b>Display the help information for ProcessStack.py:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logger - INFO - GIANT Toolbox - v 1.0\n",
      "logger - INFO - ---------------------\n",
      "usage: ProcessStack.py [-h] [-i INAME] [-o ONAME] [-x DXML] [-p PXML]\n",
      "\n",
      "Processes the stack before inversion - Deramping + Atmospheric correction.\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help  show this help message and exit\n",
      "  -i INAME    To override the default Input HDF5 file. Default: RAW-STACK.h5\n",
      "  -o ONAME    To override the default Output HDF5 file. Default: PROC-STACK.h5\n",
      "  -x DXML     To override the default data xml file. Default: data.xml\n",
      "  -p PXML     To override the default sbas xml file. Default: sbas.xml\n"
     ]
    }
   ],
   "source": [
    "!python $giant_path/ProcessStack.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>Run ProcessStack.py:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logger - INFO - GIANT Toolbox - v 1.0\n",
      "logger - INFO - ---------------------\n",
      "<module> - INFO - Input h5file: Stack/RAW-STACK.h5\n",
      "<module> - INFO - Output h5file: Stack/PROC-STACK.h5\n",
      "deramp - INFO - PROGRESS: Estimating individual ramps.\n",
      "[============================ 93% ======================>    ]      1s /      0s \n",
      "deramp - INFO - PROGRESS: Network deramp of IFGs.\n",
      "[============================ 99% =========================> ]      3s /      0s \n",
      "<module> - INFO - PNG preview of Deramped images: Figs/Ramp\n",
      "[============================ 93% ======================>    ]     37s /      2s \n"
     ]
    }
   ],
   "source": [
    "!python $giant_path/ProcessStack.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face='Calibri'><font size='3'>ProcessStack.py creates a file called 'PROC-STACK.h5'.\n",
    "<br><br>\n",
    "<b>Verify that PROC-STACK.h5 is an HDF5 file as required by the rest of GIAnT:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confirmed: /home/jovyan/GEOS_657_Labs/2019/2019/lab_9_data/Stack/PROC-STACK.h5 is an HDF5 file.\n"
     ]
    }
   ],
   "source": [
    "proc_h5 = f\"{stack_path}/PROC-STACK.h5\"\n",
    "if not h5py.is_hdf5(proc_h5):\n",
    "    print(f\"Not an HDF5 file: {proc_h5}\")\n",
    "else:\n",
    "    print(f\"Confirmed: {proc_h5} is an HDF5 file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='4'> <b>3.3 Run SBASInvert.py </b></font>\n",
    "<br>\n",
    "<font face='Calibri' size='3'> Actually do the time series. \n",
    " \n",
    "Inputs\n",
    "\n",
    "- HDF5 file, PROC-STACK.h5\n",
    "- data.xml\n",
    "- sbas.xml\n",
    "\n",
    "Outputs\n",
    "\n",
    "- HDF5 file: LS-PARAMS.h5\n",
    "\n",
    "<b>Display the help information for SBASInvert.py:</b>\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logger - INFO - GIANT Toolbox - v 1.0\n",
      "logger - INFO - ---------------------\n",
      "usage: SBASInvert.py [-h] [-i FNAME] [-o ONAME] [-d DXML] [-p PXML]\n",
      "\n",
      "Simple SBAS inversion script.\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help  show this help message and exit\n",
      "  -i FNAME    To override input HDF5 file. Default: Uses default from\n",
      "              ProcessStack.py\n",
      "  -o ONAME    To override output HDF5 file. Default: LS-PARAMS.h5\n",
      "  -d DXML     To override the data XML file. Default: data.xml\n",
      "  -p PXML     To override the processing XML file. Default: sbas.xml\n"
     ]
    }
   ],
   "source": [
    "!python $giant_path/SBASInvert.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>Run SBASInvert.py:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logger - INFO - GIANT Toolbox - v 1.0\n",
      "logger - INFO - ---------------------\n",
      "<module> - INFO - Number of interferograms = 70\n",
      "<module> - INFO - Number of unique SAR scenes = 46\n",
      "<module> - INFO - Number of connected components in network: 2\n",
      "/home/jovyan/.local/GIAnT/SCR/SBASInvert.py:154: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n",
      "  logger.warn('The network appears to be contain disconnected components\\n' +\n",
      "<module> - WARNING - The network appears to be contain disconnected components\n",
      "SBAS uses SVD to resolve the offset between components.\n",
      "Timefn - INFO - Adding 46 linear pieces (SBAS)\n",
      "<module> - INFO - Output h5file: Stack/LS-PARAMS.h5\n",
      "[============================ 99% =========================> ]     18s /      0s \n"
     ]
    }
   ],
   "source": [
    "!python $giant_path/SBASInvert.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face='Calibri'><font size='3'>SBASInvert.py creates a file called 'LS-PARAMS.h5'.\n",
    "<br><br>\n",
    "<b>Verify that LS-PARAMS.h5 is an HDF5 file as required by the rest of GIAnT:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confirmed: /home/jovyan/GEOS_657_Labs/2019/2019/lab_9_data/Stack/LS-PARAMS.h5 is an HDF5 file.\n"
     ]
    }
   ],
   "source": [
    "params_h5 = f\"{stack_path}/LS-PARAMS.h5\"\n",
    "if not h5py.is_hdf5(params_h5):\n",
    "    print(f\"Not an HDF5 file: {params_h5}\")\n",
    "else:\n",
    "    print(f\"Confirmed: {params_h5} is an HDF5 file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='4'> <b>3.4 Run SBASxval.py </b></font>\n",
    "<br>\n",
    "<font face='Calibri' size='3'> Get an uncertainty estimate for each pixel and epoch using a Jacknife test. We are skipping this function as we won't be doing anything with its output and it takes a significant amount of time to run relative to the other GIAnT functions.\n",
    " \n",
    "Inputs: \n",
    "\n",
    "- HDF5 files, PROC-STACK.h5\n",
    "- data.xml\n",
    "- sbas.xml\n",
    "\n",
    "Outputs:\n",
    "\n",
    "- HDF5 file, LS-xval.h5\n",
    "\n",
    "<br>\n",
    "<b>Display the help information for SBASxval.py:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python $giant_path/SBASxval.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>Run SBASxval.py:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python $giant_path/SBASxval.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face='Calibri'><font size='3'>SBASxval.py  creates a file called 'LS-xval.h5'.\n",
    "<br><br>\n",
    "<b>Verify that LS-xval.h5 is an HDF5 file as required by the rest of GIAnT:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "xval_h5 = f\"{stack_path}/LS-xval.h5\"\n",
    "if not h5py.is_hdf5(xval_h5):\n",
    "    print(f\"Not an HDF5 file: {xval_h5}\")\n",
    "else:\n",
    "    print(f\"Confirmed: {xval_h5} is an HDF5 file.\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='5'><b>4. Data Visualization</b></font>\n",
    "<br>\n",
    "<font face='Calibri' size='3'>Now we visualize the data. This is largely copied from Lab 4.\n",
    "<br><br>\n",
    "<b>Create a directory in which to store our plots and move into it:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: /home/jovyan/GEOS_657_Labs/2019/2019/lab_9_data/plots\n"
     ]
    }
   ],
   "source": [
    "plot_dir = f\"{path}/plots\"\n",
    "if not os.path.exists(plot_dir):\n",
    "    os.makedirs(plot_dir)\n",
    "if os.path.exists(plot_dir):\n",
    "    os.chdir(plot_dir)\n",
    "print(f\"Current Working Directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>Load the stack produced by GIAnT and read it into an array so we can manipulate and display it:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(params_h5, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>List all groups ('key's) within the HDF5 file that has been loaded into the object 'f'</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: <KeysViewHDF5 ['cmask', 'dates', 'mName', 'masterind', 'parms', 'rawts', 'recons', 'regF', 'tims']>\n"
     ]
    }
   ],
   "source": [
    "print(\"Keys: %s\" %f.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'>Details on what each of these keys means can be found in the GIAnT documentation. For now, the only keys with which we are concerned are <b>'recons'</b> (the filtered time series of each pixel) and <b>'dates'</b> (the dates of acquisition). It is important to note that the dates are given in a type of Julian Day number called Rata Die number. This will have to be converted later, but this can easily be done via one of several different methods in Python.</font>\n",
    "<br><br>\n",
    "<font face='Calibri' size='3'><b>Get our data from the stack:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cube = f['recons'][()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>Get the dates for each raster from the stack:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = list(f['dates']) # these dates appear to be given in Rata Die style: floor(Julian Day Number - 1721424.5). \n",
    "if data_cube.shape[0] is not len(dates):\n",
    "    print('Problem:')\n",
    "    print('Number of rasters in data_cube: ',data_cube.shape[0])\n",
    "    print('Number of dates: ',len(dates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>Plot and save amplitude image with transparency determined by alpha (SierraNegra-dBScaled-AmplitudeImage.png):</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 14})\n",
    "radar_tiff = f\"{path}/20161119-20170106_amp.tiff\"\n",
    "radar=gdal.Open(radar_tiff)\n",
    "im_radar = radar.GetRasterBand(1).ReadAsArray()\n",
    "radar = None\n",
    "dbplot = np.ma.log10(im_radar)\n",
    "vmin=np.percentile(dbplot,3)\n",
    "vmax=np.percentile(dbplot,97)\n",
    "fig = plt.figure(figsize=(18,10)) # Initialize figure with a size\n",
    "ax1 = fig.add_subplot(111) # 221 determines: 2 rows, 2 plots, first plot\n",
    "ax1.imshow(dbplot, cmap='gray',vmin=vmin,vmax=vmax,alpha=1);\n",
    "plt.title('Example dB-scaled SAR Image for Ifgrm 20161119-20170106')\n",
    "plt.grid()\n",
    "plt.savefig('SierraNegra-dBScaled-AmplitudeImage.png',dpi=200,transparent='false')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>Display and save an overlay of the clipped deformation map and amplitude image (SierraNegra-DeformationComposite.png):</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will define a short function that can plot an overaly of our radar image and deformation map. \n",
    "def defNradar_plot(deformation, radar):\n",
    "    fig = plt.figure(figsize=(18, 10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    vmin = np.percentile(radar, 3)\n",
    "    vmax = np.percentile(radar, 97)\n",
    "    ax.imshow(radar, cmap='gray', vmin=vmin, vmax=vmax)\n",
    "    fin_plot = ax.imshow(deformation, cmap='RdBu', vmin=-50.0, vmax=50.0, alpha=0.75)\n",
    "    fig.colorbar(fin_plot, fraction=0.24, pad=0.02)\n",
    "    ax.set(title=\"Integrated Defo [mm] Overlain on Clipped db-Scaled Amplitude Image\")\n",
    "    plt.grid()\n",
    "    \n",
    "# Get deformation map and radar image we wish to plot\n",
    "deformation = data_cube[data_cube.shape[0]-1]\n",
    "\n",
    "# Call function to plot an overlay of our deformation map and radar image.\n",
    "defNradar_plot(deformation, dbplot)\n",
    "plt.savefig('SierraNegra-DeformationComposite.png', dpi=200, transparent='false')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>Convert from Rata Die number (similar to Julian Day number) contained in 'dates' to Gregorian date:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tindex = []\n",
    "for d in dates:\n",
    "    tindex.append(date.fromordinal(int(d)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>Create an animation of the deformation</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "fig = plt.figure(figsize=(14, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.axis('off')\n",
    "vmin=np.percentile(data_cube.flatten(), 5)\n",
    "vmax=np.percentile(data_cube.flatten(), 95)\n",
    "\n",
    "\n",
    "im = ax.imshow(data_cube[0], cmap='RdBu', vmin=-50.0, vmax=50.0)\n",
    "ax.set_title(\"Animation of Deformation Time Series - Sierra Negra, Galapagos\")\n",
    "fig.colorbar(im)\n",
    "plt.grid()\n",
    "\n",
    "def animate(i):\n",
    "    ax.set_title(\"Date: {}\".format(tindex[i]))\n",
    "    im.set_data(data_cube[i])\n",
    "    \n",
    "ani = matplotlib.animation.FuncAnimation(fig, animate, frames=data_cube.shape[0], interval=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Configure matplotlib's RC settings for the animation:</b></font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc('animation', embed_limit=10.0**9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Create a javascript animation of the time-series running inline in the notebook:</b></font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Save the animation as a 'gif' file (SierraNegraDeformationTS.gif):</b></font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani.save('SierraNegraDeformationTS.gif', writer='pillow', fps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='5'><b>5. Alter the time filter parameter</b></font><br>\n",
    "    <font size='3'>Looking at the video above, you may notice that the deformation has a very smoothed appearance. This may be because of our time filter which is currently set to 1 year ('filt=1.0' in the prepxml_SBAS.py code). Let's repeat the lab from there with 2 different time filters. <br>First, using no time filter ('filt=0.0') and then using a 1 month time filter ('filt=0.082'). Change the output file name for anything you want saved (e.g., 'SierraNegraDeformationTS.gif' to 'YourDesiredFileName.gif'). Otherwise, it will be overwritten. <br><br>How did these changes affect the output time series?<br>How might we figure out the right filter length?<br>What does this say about the parameters we select?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='5'><b>6. Clear data (optional)</b></font>\n",
    "    <br>\n",
    "    <font size='3'>This lab has produced a large quantity of data. If you look at this notebook in your home directory, it should now be ~13 MB. This can take a long time to load in a Jupyter Notebook. It may be useful to clear the cell outputs. <br>To clear the cell outputs, go Cell->All Output->Clear. This will clear the outputs of the Jupyter Notebook and restore it to its original size of ~60 kB. This will not delete any of the files we have created. </font>\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"2\"> <i>GEOS 657-Lab9-InSARTimeSeriesAnalysis.ipynb - Version 1.2.0 - April 2021\n",
    "    <br>\n",
    "        <b>Version Changes:</b>\n",
    "    <ul>\n",
    "        <li>from osgeo import gdal</li>\n",
    "        <li>namespace asf_notebook</li>\n",
    "    </ul>\n",
    "    </i>\n",
    "</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "insar_analysis [conda env:.local-insar_analysis]",
   "language": "python",
   "name": "conda-env-.local-insar_analysis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
